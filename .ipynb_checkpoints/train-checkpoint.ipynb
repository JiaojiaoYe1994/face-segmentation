{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-269dd0e1f02c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "from pylab import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from dataset import CelebA\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,\n",
    "    Normalize,\n",
    "    Compose,\n",
    "    PadIfNeeded,\n",
    "    RandomCrop,\n",
    "    Rotate,\n",
    "    Resize\n",
    ")\n",
    "from models import LinkNet34\n",
    "# from fastai.vision import *\n",
    "# from fastai.callbacks.hooks import *\n",
    "# from fastai.utils.mem import *\n",
    "from torchsummary import summary\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossBinary:\n",
    "\n",
    "    def __init__(self, jaccard_weight=0):\n",
    "        self.nll_loss = nn.BCEWithLogitsLoss()\n",
    "        self.jaccard_weight = jaccard_weight\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        loss = (1 - self.jaccard_weight) * self.nll_loss(outputs, targets)\n",
    "\n",
    "        if self.jaccard_weight:\n",
    "            eps = 1e-15\n",
    "            jaccard_target = (targets == 1).float()\n",
    "            jaccard_output = F.sigmoid(outputs)\n",
    "\n",
    "            intersection = (jaccard_output * jaccard_target).sum()\n",
    "            union = jaccard_output.sum() + jaccard_target.sum()\n",
    "\n",
    "            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5635\n",
      "627\n"
     ]
    }
   ],
   "source": [
    "root = '/media/nasir/Drive1/datasets/celeba_real/'\n",
    "size = 256\n",
    "train_aug= Compose([\n",
    "            HorizontalFlip(p=0.5),\n",
    "            Rotate(15),\n",
    "         ])\n",
    "trainset = CelebA(root = root, train=True, augmentation=train_aug)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "valset = CelebA(root = root, train=False)\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(model, l):\n",
    "    total_params = 0\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "        total_params+=1\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "        if i < total_params - l:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_es = []\n",
    "def validate(net, criterion):\n",
    "    net.eval()\n",
    "    valid_loss = 0.0\n",
    "    for i, (imgs, true_masks) in enumerate(valset_loader):\n",
    "        imgs, true_masks = Variable(imgs.cuda()), Variable(true_masks.to(dtype=torch.float, device = device))\n",
    "\n",
    "\n",
    "        masks_pred = net(imgs)\n",
    "        masks_probs_flat = masks_pred.view(-1)\n",
    "        true_masks_flat = true_masks.view(-1)\n",
    "\n",
    "        loss = criterion(masks_probs_flat, true_masks_flat)\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    net.train()\n",
    "    valid_loss = valid_loss/i\n",
    "    \n",
    "    valid_loss_es.append(valid_loss)\n",
    "    best_valid_loss = min(valid_loss_es)\n",
    "\n",
    "    print(f'validation loss {round(valid_loss, 4)}, {round(best_valid_loss, 4)}')\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print('saving ... ')\n",
    "        torch.save(net.state_dict(), 'linknet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, epochs, lr=1e-2):\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, 2)\n",
    "    criterion = LossBinary(jaccard_weight=1)\n",
    "#     criterion = nn.BCELoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (imgs, true_masks) in enumerate(trainloader):\n",
    "\n",
    "            imgs, true_masks = Variable(imgs.cuda()), Variable(true_masks.to(dtype=torch.float, device = device))\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            masks_pred = model(imgs)\n",
    "            masks_probs_flat = masks_pred.view(-1)\n",
    "            true_masks_flat = true_masks.view(-1)\n",
    "            loss = criterion(masks_probs_flat, true_masks_flat)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             if i % 10 == 9:\n",
    "            sys.stdout.write(f'\\rEpoch: {epoch} ---- Loss: {round(epoch_loss/(i+1), 4)}')\n",
    "            sys.stdout.flush()\n",
    "            cosine.step()\n",
    "            \n",
    "        sys.stdout.write('\\n')\n",
    "        validate(model, criterion)\n",
    "        sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkNet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-11           [-1, 64, 64, 64]               0\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "             ReLU-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 64, 64]             128\n",
      "             ReLU-17           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-18           [-1, 64, 64, 64]               0\n",
      "           Conv2d-19           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 64, 64]             128\n",
      "             ReLU-21           [-1, 64, 64, 64]               0\n",
      "           Conv2d-22           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 64, 64]             128\n",
      "             ReLU-24           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-25           [-1, 64, 64, 64]               0\n",
      "           Conv2d-26          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
      "             ReLU-28          [-1, 128, 32, 32]               0\n",
      "           Conv2d-29          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 32, 32]             256\n",
      "           Conv2d-31          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-34          [-1, 128, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 32, 32]             256\n",
      "             ReLU-40          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-41          [-1, 128, 32, 32]               0\n",
      "           Conv2d-42          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 32, 32]             256\n",
      "             ReLU-44          [-1, 128, 32, 32]               0\n",
      "           Conv2d-45          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 32, 32]             256\n",
      "             ReLU-47          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-48          [-1, 128, 32, 32]               0\n",
      "           Conv2d-49          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 32, 32]             256\n",
      "             ReLU-51          [-1, 128, 32, 32]               0\n",
      "           Conv2d-52          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 32, 32]             256\n",
      "             ReLU-54          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-55          [-1, 128, 32, 32]               0\n",
      "           Conv2d-56          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
      "             ReLU-58          [-1, 256, 16, 16]               0\n",
      "           Conv2d-59          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 16, 16]             512\n",
      "           Conv2d-61          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 16, 16]             512\n",
      "             ReLU-63          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-64          [-1, 256, 16, 16]               0\n",
      "           Conv2d-65          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 16, 16]             512\n",
      "             ReLU-67          [-1, 256, 16, 16]               0\n",
      "           Conv2d-68          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 16, 16]             512\n",
      "             ReLU-70          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-71          [-1, 256, 16, 16]               0\n",
      "           Conv2d-72          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 16, 16]             512\n",
      "             ReLU-74          [-1, 256, 16, 16]               0\n",
      "           Conv2d-75          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 16, 16]             512\n",
      "             ReLU-77          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-78          [-1, 256, 16, 16]               0\n",
      "           Conv2d-79          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 16, 16]             512\n",
      "             ReLU-81          [-1, 256, 16, 16]               0\n",
      "           Conv2d-82          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 16, 16]             512\n",
      "             ReLU-84          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-85          [-1, 256, 16, 16]               0\n",
      "           Conv2d-86          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 16, 16]             512\n",
      "             ReLU-88          [-1, 256, 16, 16]               0\n",
      "           Conv2d-89          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 16, 16]             512\n",
      "             ReLU-91          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-92          [-1, 256, 16, 16]               0\n",
      "           Conv2d-93          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 16, 16]             512\n",
      "             ReLU-95          [-1, 256, 16, 16]               0\n",
      "           Conv2d-96          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 16, 16]             512\n",
      "             ReLU-98          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-99          [-1, 256, 16, 16]               0\n",
      "          Conv2d-100            [-1, 512, 8, 8]       1,179,648\n",
      "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-102            [-1, 512, 8, 8]               0\n",
      "          Conv2d-103            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-105            [-1, 512, 8, 8]         131,072\n",
      "     BatchNorm2d-106            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-107            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-108            [-1, 512, 8, 8]               0\n",
      "          Conv2d-109            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-110            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-111            [-1, 512, 8, 8]               0\n",
      "          Conv2d-112            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-114            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-115            [-1, 512, 8, 8]               0\n",
      "          Conv2d-116            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-117            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-118            [-1, 512, 8, 8]               0\n",
      "          Conv2d-119            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-121            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-122            [-1, 512, 8, 8]               0\n",
      "          Conv2d-123            [-1, 128, 8, 8]          65,664\n",
      "     BatchNorm2d-124            [-1, 128, 8, 8]             256\n",
      "            ReLU-125            [-1, 128, 8, 8]               0\n",
      " ConvTranspose2d-126          [-1, 128, 16, 16]         262,272\n",
      "     BatchNorm2d-127          [-1, 128, 16, 16]             256\n",
      "            ReLU-128          [-1, 128, 16, 16]               0\n",
      "          Conv2d-129          [-1, 256, 16, 16]          33,024\n",
      "     BatchNorm2d-130          [-1, 256, 16, 16]             512\n",
      "            ReLU-131          [-1, 256, 16, 16]               0\n",
      "DecoderBlockLinkNet-132          [-1, 256, 16, 16]               0\n",
      "          Conv2d-133           [-1, 64, 16, 16]          16,448\n",
      "     BatchNorm2d-134           [-1, 64, 16, 16]             128\n",
      "            ReLU-135           [-1, 64, 16, 16]               0\n",
      " ConvTranspose2d-136           [-1, 64, 32, 32]          65,600\n",
      "     BatchNorm2d-137           [-1, 64, 32, 32]             128\n",
      "            ReLU-138           [-1, 64, 32, 32]               0\n",
      "          Conv2d-139          [-1, 128, 32, 32]           8,320\n",
      "     BatchNorm2d-140          [-1, 128, 32, 32]             256\n",
      "            ReLU-141          [-1, 128, 32, 32]               0\n",
      "DecoderBlockLinkNet-142          [-1, 128, 32, 32]               0\n",
      "          Conv2d-143           [-1, 32, 32, 32]           4,128\n",
      "     BatchNorm2d-144           [-1, 32, 32, 32]              64\n",
      "            ReLU-145           [-1, 32, 32, 32]               0\n",
      " ConvTranspose2d-146           [-1, 32, 64, 64]          16,416\n",
      "     BatchNorm2d-147           [-1, 32, 64, 64]              64\n",
      "            ReLU-148           [-1, 32, 64, 64]               0\n",
      "          Conv2d-149           [-1, 64, 64, 64]           2,112\n",
      "     BatchNorm2d-150           [-1, 64, 64, 64]             128\n",
      "            ReLU-151           [-1, 64, 64, 64]               0\n",
      "DecoderBlockLinkNet-152           [-1, 64, 64, 64]               0\n",
      "          Conv2d-153           [-1, 16, 64, 64]           1,040\n",
      "     BatchNorm2d-154           [-1, 16, 64, 64]              32\n",
      "            ReLU-155           [-1, 16, 64, 64]               0\n",
      " ConvTranspose2d-156         [-1, 16, 128, 128]           4,112\n",
      "     BatchNorm2d-157         [-1, 16, 128, 128]              32\n",
      "            ReLU-158         [-1, 16, 128, 128]               0\n",
      "          Conv2d-159         [-1, 64, 128, 128]           1,088\n",
      "     BatchNorm2d-160         [-1, 64, 128, 128]             128\n",
      "            ReLU-161         [-1, 64, 128, 128]               0\n",
      "DecoderBlockLinkNet-162         [-1, 64, 128, 128]               0\n",
      " ConvTranspose2d-163         [-1, 32, 257, 257]          18,464\n",
      "            ReLU-164         [-1, 32, 257, 257]               0\n",
      "          Conv2d-165         [-1, 32, 255, 255]           9,248\n",
      "            ReLU-166         [-1, 32, 255, 255]               0\n",
      "          Conv2d-167          [-1, 1, 256, 256]             129\n",
      "================================================================\n",
      "Total params: 21,794,721\n",
      "Trainable params: 21,794,721\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 250.31\n",
      "Params size (MB): 83.14\n",
      "Estimated Total Size (MB): 334.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.train()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('linknet.pth'))\n",
    "\n",
    "summary(model, (3, 256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, trainloader, 5, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, trainloader, 5, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, trainloader, 10, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
